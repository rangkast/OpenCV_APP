import bpy
import bgl
import pickle
import gzip
import numpy as np
import bmesh
import mathutils
import math
import bpy_extras
from mathutils import Vector, geometry
import cv2
import os
from mathutils import Matrix
from bpy_extras import mesh_utils
from math import degrees
from datetime import datetime
import platform
import json
from collections import OrderedDict
from scipy.spatial.transform import Rotation as Rot


READ = 0
WRITE = 1

ERROR = 'ERROR'
SUCCESS = 'SUCCESS'

DONE = 'DONE'
NOT_SET = 'NOT_SET'


'''
Definitions
'''
camera_names = ["CAMERA_0", "CAMERA_1"]

padding = 0.0  # 원하는 패딩 값을 입력하세요.
# LED 원의 반지름을 설정합니다. 원하는 크기를 입력으로 제공할 수 있습니다.
#led_size = 0.0025
led_size = 0.0025
led_thickness = 0.0022
emission_strength = 0.01


default_cameraK = {'serial': "default", 'camera_f': [1, 1], 'camera_c': [0, 0]}
default_dist_coeffs = np.zeros((4, 1))
cam_0_matrix = {'serial': "WMTD306N100AXM", 'camera_f': [712.623, 712.623], 'camera_c': [653.448, 475.572], 'dist_coeff': np.array([[0.072867], [-0.026268], [0.007135], [-0.000997]], dtype=np.float64)}
cam_1_matrix = {'serial': "WMTD305L6003D6", 'camera_f': [716.896, 716.896], 'camera_c': [668.902, 460.618], 'dist_coeff': np.array([[0.07542], [-0.026874], [0.006662], [-0.000775]], dtype=np.float64)}


'''
Simulation shampe
'''
# shape = 'plane'
# shape = 'sphere'
# shape = 'cylinder'
#shape = 'cylinder_base'
#shape = 'basic'

'''
Real UVC Camera
'''
shape = 'real'


MESH_OBJ_NAME = 'MeshObject_' + f'{shape}'

model_pickle_file = None
camera_pickle_file = None
os_name = platform.system()
if os_name == 'Windows':
    print("This is Windows")
    if shape == 'sphere':
        model_pickle_file = 'D:/OpenCV_APP/led_pos_simulation/find_pos_legacy/result.pickle'
    elif shape == 'cylinder':
        model_pickle_file = 'D:/OpenCV_APP/led_pos_simulation/find_pos_legacy/result_cylinder.pickle'
    elif shape == 'cylinder_base':
        model_pickle_file = 'D:/OpenCV_APP/led_pos_simulation/find_pos_legacy/result_cylinder_base.pickle'
        base_file_path = 'D:/OpenCV_APP/led_pos_simulation/blender_3d/image_output/cylinder_base/'
    elif shape == 'real':
        base_file_path = 'D:/OpenCV_APP/led_pos_simulation/blender_3d/image_output/real_image/'
    else:
        model_pickle_file = 'D:/OpenCV_APP/led_pos_simulation/find_pos_legacy/basic_test.pickle'
        base_file_path = 'D:/OpenCV_APP/led_pos_simulation/blender_3d/image_output/blender_basic/'
    
    camera_pickle_file = 'D:/OpenCV_APP/led_pos_simulation/blender_3d/real_camera_data.pickle'

elif os_name == 'Linux':
    print("This is Linux")
    if shape == 'sphere':
        model_pickle_file = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/find_pos_legacy/result.pickle'
    elif shape == 'cylinder':
        model_pickle_file = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/find_pos_legacy/result_cylinder.pickle'
    elif shape == 'cylinder_base':
        model_pickle_file = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/find_pos_legacy/result_cylinder_base.pickle'
        base_file_path = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/blender_3d/image_output/cylinder_base/'
    elif shape == 'real':
        base_file_path = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/blender_3d/image_output/real_image/'
    else:
        model_pickle_file = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/find_pos_legacy/basic_test.pickle'
        base_file_path = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/blender_3d/image_output/blender_basic/'
    
    camera_pickle_file = '/home/rangkast.jeong/Project/OpenCV_APP/led_pos_simulation/blender_3d/real_camera_data.pickle'
else:
    print("Unknown OS")

image_file_path = base_file_path
blend_file_path = base_file_path + 'blender_test_image.blend'


# Default
default_rvec_left = np.array([1.0, 0, 0])
default_tvec_left = np.array([0, 0, 0])

default_rvec_right = np.array([1.0, 0, 0])
default_tvec_right = np.array([0, 0, 0])

#Custom Camera Posion
rvec_left = np.array([ 1.2091998,   1.20919946, -1.20919946])
tvec_left = np.array([-2.82086248e-08, -2.35607960e-08,  2.00000048e-01])

rvec_right = np.array([ 0.86044094,  1.63833515, -1.63833502])
tvec_right = np.array([-7.45058060e-08, -1.88919191e-08,  2.00000152e-01])

origin_led_data = np.array([
    [-0.02146761, -0.00343424, -0.01381839],
    [-0.0318701, 0.00568587, -0.01206734],
    [-0.03692925, 0.00930785, 0.00321071],
    [-0.04287211, 0.02691347, -0.00194137],
    [-0.04170018, 0.03609551, 0.01989264],
    [-0.02923584, 0.06186962, 0.0161972],
    [-0.01456789, 0.06295633, 0.03659283],
    [0.00766914, 0.07115411, 0.0206431],
    [0.02992447, 0.05507271, 0.03108736],
    [0.03724313, 0.05268665, 0.01100446],
    [0.04265723, 0.03016438, 0.01624689],
    [0.04222733, 0.0228845, -0.00394005],
    [0.03300807, 0.00371497, 0.00026865],
    [0.03006234, 0.00378822, -0.01297127],
    [0.02000199, -0.00388647, -0.014973]
])

origin_led_dir = np.array([
    [-0.52706841, -0.71386452, -0.46108171],
    [-0.71941994, -0.53832866, -0.43890456],
    [-0.75763735, -0.6234486, 0.19312559],
    [-0.95565641, 0.00827838, -0.29436762],
    [-0.89943476, -0.04857372, 0.43434745],
    [-0.57938915, 0.80424722, -0.13226727],
    [-0.32401356, 0.5869508, 0.74195955],
    [0.14082806, 0.97575588, -0.16753482],
    [0.66436362, 0.41503629, 0.62158335],
    [0.77126662, 0.61174447, -0.17583089],
    [0.90904575, -0.17393345, 0.37865945],
    [0.9435189, -0.10477919, -0.31431419],
    [0.7051038, -0.6950803, 0.14032818],
    [0.67315478, -0.5810967, -0.45737213],
    [0.49720891, -0.70839529, -0.5009585]
])



'''
Functions
'''

def delete_all_objects_except(exclude_object_names):
    for obj in bpy.data.objects:
        if obj.name not in exclude_object_names:
            bpy.data.objects.remove(obj, do_unlink=True)


def create_mesh_object(coords, name, padding=0.0):
    # 새로운 메시를 생성하고 이름을 설정합니다.
    mesh = bpy.data.meshes.new(name)
    obj = bpy.data.objects.new(name, mesh)

    # 씬에 객체를 추가합니다.
    scene = bpy.context.scene
    scene.collection.objects.link(obj)

    # bmesh를 사용하여 메시를 생성합니다.
    bm = bmesh.new()

    # 좌표에 꼭짓점을 추가합니다.
    verts = [bm.verts.new(coord) for coord in coords]

    # Convex Hull 알고리즘을 사용하여 표면을 생성합니다.
    bmesh.ops.convex_hull(bm, input=verts)

    # 변경 사항을 메시에 적용합니다.
    bm.to_mesh(mesh)
    bm.free()

    # 위아래 패딩을 적용합니다.
    if padding != 0.0:
        bounding_box = obj.bound_box
        z_min = min([point[2] for point in bounding_box])
        z_max = max([point[2] for point in bounding_box])

        # Z 축에만 패딩을 적용하기 위해 Z 좌표를 수정합니다.
        for v in mesh.vertices:
            v.co.z += padding if v.co.z > 0 else -padding

    # 짙은 회색 반투명 재질 생성
    mesh_material = bpy.data.materials.new(name="Mesh_Material")
    mesh_material.use_nodes = True

    # 노드를 수정하여 블렌드 모드 설정
    nodes = mesh_material.node_tree.nodes
    links = mesh_material.node_tree.links

    # 기존 Principled BSDF 노드와 Material Output 노드를 가져옵니다.
    principled_node = nodes.get("Principled BSDF")
    output_node = nodes.get("Material Output")

    # 기존 Principled BSDF 노드의 설정을 변경합니다.
    principled_node.inputs["Base Color"].default_value = (0.1, 0.1, 0.1, 1)  # 짙은 회색
    principled_node.inputs["Alpha"].default_value = 1.0  # 알파 값 변경 (1.0로 설정)
    principled_node.inputs["Transmission"].default_value = 1.0  # 빛 투과값 설정 (1.0로 설정)
    principled_node.inputs["IOR"].default_value = 1.0  # 굴절률 설정 (1.0로 설정)

    # 블렌드 모드 설정
    #    mesh_material.blend_method = 'BLEND'  # 블렌드 모드 설정
    #    mesh_material.shadow_method = 'NONE'  # 그림자 방법 설정

    # 이제 수정한 Principled BSDF 노드와 Material Output 노드를 연결합니다.
    links.new(principled_node.outputs["BSDF"], output_node.inputs["Surface"])

    # 메시 오브젝트에 반투명 재질 적용
    obj.data.materials.append(mesh_material)

    # 메시 스무딩 설정
    for p in mesh.polygons:
        p.use_smooth = True

    return obj


def create_circle_leds_on_surface(led_coords, led_size, shape, name_prefix="LED"):
    led_objects = []

    for i, coord in enumerate(led_coords):
        # LED 오브젝트의 위치를 조정합니다.
        normalized_direction = Vector(coord).normalized()
        if shape == 'sphere':
            distance_to_o = led_size * 2 / 3
            location = [coord[0] - distance_to_o * normalized_direction.x,
                        coord[1] - distance_to_o * normalized_direction.y,
                        coord[2] - distance_to_o * normalized_direction.z]
        else:
            location = coord

        bpy.ops.mesh.primitive_uv_sphere_add(segments=32, ring_count=16, radius=led_size, location=location)
        led_obj = bpy.context.active_object
        led_obj.name = f"{name_prefix}_{i}"

        # 노드 기반의 LED 재질을 설정합니다.
        led_material = bpy.data.materials.new(name=f"LED_Material_{i}")
        led_material.use_nodes = True
        led_material.node_tree.nodes.clear()

        # Emission 쉐이더를 추가합니다.
        nodes = led_material.node_tree.nodes
        links = led_material.node_tree.links
        output_node = nodes.new(type='ShaderNodeOutputMaterial')
        emission_node = nodes.new(type='ShaderNodeEmission')

        # Emission 쉐이더의 강도와 색상을 설정합니다.
        emission_node.inputs['Strength'].default_value = emission_strength  # 강도 조절
        emission_node.inputs['Color'].default_value = (255, 255, 255, 1)  # 색상 조절

        # Emission 쉐이더를 출력 노드에 연결합니다.
        links.new(emission_node.outputs['Emission'], output_node.inputs['Surface'])

        led_obj.data.materials.append(led_material)

        led_objects.append(led_obj)

    return led_objects


def set_up_dark_world_background():
    # 월드 배경을 어둡게 설정합니다.
    world = bpy.context.scene.world
    world.use_nodes = True
    bg_node = world.node_tree.nodes["Background"]
    bg_node.inputs["Color"].default_value = (0, 0, 0, 1)  # 검은색 배경
    bg_node.inputs["Strength"].default_value = 0.5


# Rotation Matrix to Euler Angles (XYZ)
def rotation_matrix_to_euler_angles(R):
    sy = np.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])
    singular = sy < 1e-6

    if not singular:
        x = np.arctan2(R[2, 1], R[2, 2])
        y = np.arctan2(-R[2, 0], sy)
        z = np.arctan2(R[1, 0], R[0, 0])
    else:
        x = np.arctan2(-R[1, 2], R[1, 1])
        y = np.arctan2(-R[2, 0], sy)
        z = 0

    return np.array([x, y, z])


def rotation_matrix_to_quaternion(R):
    qw = np.sqrt(1 + R[0, 0] + R[1, 1] + R[2, 2]) / 2
    qx = (R[2, 1] - R[1, 2]) / (4 * qw)
    qy = (R[0, 2] - R[2, 0]) / (4 * qw)
    qz = (R[1, 0] - R[0, 1]) / (4 * qw)

    return np.array([qw, qx, qy, qz])




def DeselectEdgesAndPolygons(obj):
    for p in obj.data.polygons:
        p.select = False
    for e in obj.data.edges:
        e.select = False


def get_sensor_size(sensor_fit, sensor_x, sensor_y):
    if sensor_fit == 'VERTICAL':
        return sensor_y
    return sensor_x


# BKE_camera_sensor_fit
def get_sensor_fit(sensor_fit, size_x, size_y):
    if sensor_fit == 'AUTO':
        if size_x >= size_y:
            return 'HORIZONTAL'
        else:
            return 'VERTICAL'
    return sensor_fit


# Build intrinsic camera parameters from Blender camera data
#
# See notes on this in 
# blender.stackexchange.com/questions/15102/what-is-blenders-camera-projection-matrix-model
# as well as
# https://blender.stackexchange.com/a/120063/3581
def get_calibration_matrix_K_from_blender(camd):
    if camd.type != 'PERSP':
        raise ValueError('Non-perspective cameras not supported')
    scene = bpy.context.scene
    f_in_mm = camd.lens
    scale = scene.render.resolution_percentage / 100
    resolution_x_in_px = scale * scene.render.resolution_x
    resolution_y_in_px = scale * scene.render.resolution_y
    sensor_size_in_mm = get_sensor_size(camd.sensor_fit, camd.sensor_width, camd.sensor_height)
    sensor_fit = get_sensor_fit(
        camd.sensor_fit,
        scene.render.pixel_aspect_x * resolution_x_in_px,
        scene.render.pixel_aspect_y * resolution_y_in_px
    )
    pixel_aspect_ratio = scene.render.pixel_aspect_y / scene.render.pixel_aspect_x
    if sensor_fit == 'HORIZONTAL':
        view_fac_in_px = resolution_x_in_px
    else:
        view_fac_in_px = pixel_aspect_ratio * resolution_y_in_px
    pixel_size_mm_per_px = sensor_size_in_mm / f_in_mm / view_fac_in_px
    s_u = 1 / pixel_size_mm_per_px
    s_v = 1 / pixel_size_mm_per_px / pixel_aspect_ratio

    # Parameters of intrinsic calibration matrix K
    u_0 = resolution_x_in_px / 2 - camd.shift_x * view_fac_in_px
    v_0 = resolution_y_in_px / 2 + camd.shift_y * view_fac_in_px / pixel_aspect_ratio
    skew = 0  # only use rectangular pixels

    K = Matrix(
        ((s_u, skew, u_0),
         (0, s_v, v_0),
         (0, 0, 1)))

    print('sensor_fit', sensor_fit)
    print('f_in_mm', f_in_mm)
    print('sensor_size_in_mm', sensor_size_in_mm)
    print('res_x res_y', resolution_x_in_px, resolution_y_in_px)
    print('pixel_aspect_ratio', pixel_aspect_ratio)
    print('shift_x shift_y', camd.shift_x, camd.shift_y)
    print('K', K)

    return K


# Returns camera rotation and translation matrices from Blender.
# 
# There are 3 coordinate systems involved:
#    1. The World coordinates: "world"
#       - right-handed
#    2. The Blender camera coordinates: "bcam"
#       - x is horizontal
#       - y is up
#       - right-handed: negative z look-at direction
#    3. The desired computer vision camera coordinates: "cv"
#       - x is horizontal
#       - y is down (to align to the actual pixel coordinates 
#         used in digital images)
#       - right-handed: positive z look-at direction
def get_3x4_RT_matrix_from_blender(cam):
    # bcam stands for blender camera
    R_bcam2cv = Matrix(
        ((1, 0, 0),
         (0, -1, 0),
         (0, 0, -1)))

    # Transpose since the rotation is object rotation, 
    # and we want coordinate rotation
    # R_world2bcam = cam.rotation_euler.to_matrix().transposed()
    # T_world2bcam = -1*R_world2bcam @ location
    #
    # Use matrix_world instead to account for all constraints
    location, rotation = cam.matrix_world.decompose()[0:2]
    R_world2bcam = rotation.to_matrix().transposed()

    # Convert camera location to translation vector used in coordinate changes
    # T_world2bcam = -1*R_world2bcam @ cam.location
    # Use location from matrix_world to account for constraints:     
    T_world2bcam = -1 * R_world2bcam @ location

    # Build the coordinate transform matrix from world to computer vision camera
    R_world2cv = R_bcam2cv @ R_world2bcam
    T_world2cv = R_bcam2cv @ T_world2bcam

    # put into 3x4 matrix
    RT = Matrix((
        R_world2cv[0][:] + (T_world2cv[0],),
        R_world2cv[1][:] + (T_world2cv[1],),
        R_world2cv[2][:] + (T_world2cv[2],)
    ))
    return RT


def get_3x4_RT_matrix_from_blender_OpenCV(obj):
    isCamera = (obj.type == 'CAMERA')
    R_BlenderView_to_OpenCVView = np.diag([1 if isCamera else -1, -1, -1])
    print('R_BlenderView_to_OpenCVView', R_BlenderView_to_OpenCVView)
    location, rotation = obj.matrix_world.decompose()[:2]
    print('location', location, 'rotation', rotation)
    # Convert the rotation to axis-angle representation
    axis, angle = rotation.to_axis_angle()

    # Create a 3x3 rotation matrix from the axis-angle representation
    R_BlenderView = Matrix.Rotation(angle, 3, axis).transposed()

    T_BlenderView = -1.0 * R_BlenderView @ location

    R_OpenCV = R_BlenderView_to_OpenCVView @ R_BlenderView
    T_OpenCV = R_BlenderView_to_OpenCVView @ T_BlenderView

    R, _ = cv2.Rodrigues(R_OpenCV)
    print('R_OpenCV', R_OpenCV)
    print('R_OpenCV(Rod)', R.ravel())
    print('T_OpenCV', T_OpenCV)

    RT_OpenCV = Matrix(np.column_stack((R_OpenCV, T_OpenCV)))
    return RT_OpenCV, R_OpenCV, T_OpenCV


def get_3x4_P_matrix_from_blender_OpenCV(cam):
    K = get_calibration_matrix_K_from_blender(cam.data)
    RT = get_3x4_RT_matrix_from_blender_OpenCV(cam)[0]
    return K @ RT


def get_3x4_P_matrix_from_blender(cam):
    K = get_calibration_matrix_K_from_blender(cam.data)
    RT = get_3x4_RT_matrix_from_blender(cam)
    return K @ RT, K, RT



def blender_location_rotation_from_opencv(rvec, tvec, isCamera=True):
    R_BlenderView_to_OpenCVView = Matrix([
        [1 if isCamera else -1, 0, 0],
        [0, -1, 0],
        [0, 0, -1],
    ])

    # Convert rvec to rotation matrix
    R_OpenCV, _ = cv2.Rodrigues(rvec)

    # Convert OpenCV R|T to Blender R|T
    R_BlenderView = R_BlenderView_to_OpenCVView @ Matrix(R_OpenCV.tolist())
    T_BlenderView = R_BlenderView_to_OpenCVView @ Vector(tvec)

    # Invert rotation matrix
    R_BlenderView_inv = R_BlenderView.transposed()

    # Calculate location
    location = -1.0 * R_BlenderView_inv @ T_BlenderView

    # Convert rotation matrix to quaternion
    rotation = R_BlenderView_inv.to_quaternion()

    return location, rotation


# ----------------------------------------------------------
# Alternate 3D coordinates to 2D pixel coordinate projection code
# adapted from https://blender.stackexchange.com/questions/882/how-to-find-image-coordinates-of-the-rendered-vertex?lq=1
# to have the y axes pointing up and origin at the top-left corner
def project_by_object_utils(cam, point):
    scene = bpy.context.scene
    co_2d = bpy_extras.object_utils.world_to_camera_view(scene, cam, point)
    print('co_2d', co_2d)
    render_scale = scene.render.resolution_percentage / 100
    render_size = (
        int(scene.render.resolution_x * render_scale),
        int(scene.render.resolution_y * render_scale),
    )
    return (co_2d.x * render_size[0], render_size[1] - co_2d.y * render_size[1])


def quaternion_to_euler_degree(quaternion):
    # Convert quaternion to Euler rotation (radians)
    euler_rad = quaternion.to_euler()

    # Convert radians to degrees
    euler_deg = Vector([math.degrees(axis) for axis in euler_rad])

    return euler_deg


def calculate_camera_position_direction(rvec, tvec):
    # Extract rotation matrix
    R, _ = cv2.Rodrigues(rvec)
    R_inv = np.linalg.inv(R)
    # Camera position (X, Y, Z)
    Cam_pos = -R_inv @ tvec
    X, Y, Z = Cam_pos.ravel()
    unit_z = np.array([0, 0, 1])

    # euler_angles = rotation_matrix_to_euler_angles(R)
    # print("Euler angles (in degrees):", euler_angles)

    # idea 1
    # cosine_for_pitch = math.sqrt(R[0][0] ** 2 + R[1][0] ** 2)
    # is_singular = cosine_for_pitch < 10**-6
    # if not is_singular:
    #     print('not singular')
    #     roll = math.atan2(R[2][1], R[2][2])
    #     pitch = math.atan2(-R[2][0], math.sqrt(R[2][1]**2 + R[2][2]**2))
    #     yaw = math.atan2(R[1][0], R[0][0])
    # else:
    #     print('singular')
    #     yaw = math.atan2(-R[1][2], R[1][1])
    #     pitch = math.atan2(-R[2][0], cosine_for_pitch)
    #     roll = 0

    # idea 2
    Zc = np.reshape(unit_z, (3, 1))
    Zw = np.dot(R_inv, Zc)  # world coordinate of optical axis
    zw = Zw.ravel()

    pan = np.arctan2(zw[1], zw[0]) - np.pi / 2
    tilt = np.arctan2(zw[2], np.sqrt(zw[0] * zw[0] + zw[1] * zw[1]))

    # roll
    unit_x = np.array([1, 0, 0])
    Xc = np.reshape(unit_x, (3, 1))
    Xw = np.dot(R_inv, Xc)  # world coordinate of camera X axis
    xw = Xw.ravel()
    xpan = np.array([np.cos(pan), np.sin(pan), 0])

    roll = np.arccos(np.dot(xw, xpan))  # inner product
    if xw[2] < 0:
        roll = -roll

    roll = math.degrees(roll)
    pan = math.degrees(pan)
    tilt = math.degrees(tilt)

    print('degrees', 'roll', roll, 'pan', pan, 'tilt', tilt)

    optical_axis = R.T @ unit_z.T
    # 카메라 위치에서 optical axis까지의 방향 벡터 계산
    optical_axis_x, optical_axis_y, optical_axis_z = optical_axis

    return (X, Y, Z), (optical_axis_x, optical_axis_y, optical_axis_z), (roll, pan, tilt)


def get_sensor_fit(sensor_fit, size_x, size_y):
    if sensor_fit == 'AUTO':
        if size_x >= size_y:
            return 'HORIZONTAL'
        else:
            return 'VERTICAL'
    return sensor_fit


#def create_camera(camera_f, camera_c, cam_location, name, rot):
#    rotation = quaternion_to_euler_degree(rot)
#    print('euler(degree)', rotation)

#    X, Y, Z = cam_location
#    print('position', X, Y, Z)

#    if name in bpy.data.objects:
#        bpy.data.objects.remove(bpy.data.objects[name], do_unlink=True)

#    bpy.ops.object.camera_add(location=(X, Y, Z))

#    cam = bpy.context.active_object
#    cam.name = name
#    cam.rotation_euler = (math.radians(rotation[0]), math.radians(rotation[1]), math.radians(rotation[2]))
#    
#    fx_px, fy_px = camera_f
#    cx_px, cy_px = camera_c

#    # Set the camera sensor size in pixels
#    sensor_width_px = 1280.0
#    sensor_height_px = 960.0

#    # Calculate the sensor size in millimeters
#    sensor_width_mm = 36  # Assuming 35mm camera sensor size
##    sensor_height_mm = 27.0  # Assuming 35mm camera sensor size
#    scale = sensor_width_px / sensor_width_mm  # Pixel per mm scale factor
#    sensor_width = sensor_width_px / scale
#    sensor_height = sensor_height_px / scale

#    # Calculate the focal length in millimeters
#    fx_mm = fx_px / scale
#    fy_mm = fy_px / scale
#    focal_length = (fx_mm + fy_mm) / 2.0

#    # Calculate the image center in pixels
#    cx = sensor_width_px / 2.0 + cx_px / scale
#    cy = sensor_height_px / 2.0 - cy_px / scale

#    # Set the camera parameters
#    cam.data.type = 'PERSP'
#    cam.data.lens_unit = 'FOV'
#    cam.data.angle = 2 * math.atan(sensor_width / (2 * focal_length))  # Field of view in radians

#    cam.data.sensor_width = sensor_width
#    cam.data.sensor_height = sensor_height

#    scene = bpy.context.scene

#    pixel_aspect_ratio = scene.render.pixel_aspect_y / scene.render.pixel_aspect_x

#    view_fac_in_px = sensor_width_px

#    cam.data.shift_x = (sensor_width_px / 2 - cx_px) / view_fac_in_px
#    cam.data.shift_y = (cy_px - sensor_height_px / 2) * pixel_aspect_ratio / view_fac_in_px

#    print('shift_x, shift_y', cam.data.shift_x, cam.data.shift_y)
#    return cam


def create_camera(camera_f, camera_c, cam_location, name, rot):
    rotation = quaternion_to_euler_degree(rot)
    print('euler(degree)', rotation)

    X, Y, Z = cam_location
    print('position', X, Y, Z)

    if name in bpy.data.objects:
        bpy.data.objects.remove(bpy.data.objects[name], do_unlink=True)

    bpy.ops.object.camera_add(location=(X, Y, Z))

    cam = bpy.context.active_object
    cam.name = name
    cam.rotation_euler = (math.radians(rotation[0]), math.radians(rotation[1]), math.radians(rotation[2]))
    
    fx_px, fy_px = camera_f
    cx_px, cy_px = camera_c

    # Set the camera sensor size in pixels
    sensor_width_px = 1280.0
    sensor_height_px = 960.0

    # Calculate the sensor size in millimeters
    sensor_width_mm = 7.18  # Set camera sensor width to 8mm
    sensor_height_mm = 5.32 # Set camera sensor height to 8mm
    scale = sensor_width_px / sensor_width_mm  # Pixel per mm scale factor
    sensor_width = sensor_width_px / scale
    sensor_height = sensor_height_px / scale

    # Calculate the focal length in millimeters
    fx_mm = fx_px / scale
    fy_mm = fy_px / scale
    focal_length = (fx_mm + fy_mm) / 2.0

    # Calculate the image center in pixels
    cx = sensor_width_px / 2.0 + cx_px / scale
    cy = sensor_height_px / 2.0 - cy_px / scale

    # Set the camera parameters
    cam.data.type = 'PERSP'
    cam.data.lens_unit = 'MILLIMETERS'
    cam.data.lens = focal_length  # Set focal length

    cam.data.sensor_width = sensor_width_mm
    cam.data.sensor_height = sensor_height_mm

    scene = bpy.context.scene
    pixel_aspect_ratio = scene.render.pixel_aspect_y / scene.render.pixel_aspect_x
    view_fac_in_px = sensor_width_px

    cam.data.shift_x = (sensor_width_px / 2 - cx_px) / view_fac_in_px
    cam.data.shift_y = (cy_px - sensor_height_px / 2) * pixel_aspect_ratio / view_fac_in_px

    print('shift_x, shift_y', cam.data.shift_x, cam.data.shift_y)
    return cam



def create_camera_default(cam_location, rot, name):
    # Set the camera sensor size in pixels
    sensor_width_px = 1280.0
    sensor_height_px = 960.0

    rotation = quaternion_to_euler_degree(rot)
    print('euler(degree)', rotation)

    X, Y, Z = cam_location
    print('position', X, Y, Z)
    # Remove existing camera
    if name in bpy.data.objects:
        bpy.data.objects.remove(bpy.data.objects[name], do_unlink=True)

    # MAKE DEFAULT CAM
    bpy.ops.object.camera_add(location=(X, Y, Z))
    cam = bpy.context.active_object
    cam.name = name
    cam.rotation_euler = (math.radians(rotation[0]), math.radians(rotation[1]), math.radians(rotation[2]))


def create_point(location, point_name="Point"):
    mesh = bpy.data.meshes.new(point_name)
    obj = bpy.data.objects.new(point_name, mesh)

    bpy.context.collection.objects.link(obj)
    bpy.context.view_layer.objects.active = obj
    obj.select_set(True)

    bm = bmesh.new()
    bmesh.ops.create_vert(bm, co=location)
    bm.to_mesh(mesh)
    bm.free()

    return obj


def create_direction_point(start_location, direction, point_name="DirectionPoint", scale=1.0, color=(1, 0, 0, 1)):
    mesh = bpy.data.meshes.new(point_name)
    obj = bpy.data.objects.new(point_name, mesh)

    bpy.context.collection.objects.link(obj)
    bpy.context.view_layer.objects.active = obj
    obj.select_set(True)

    bm = bmesh.new()
    end_vert = bm.verts.new(mathutils.Vector(start_location) + mathutils.Vector(direction) * scale)

    bm.to_mesh(mesh)
    bm.free()

    # Set material and color
    mat = bpy.data.materials.new(name="DirectionPointMat")
    mat.diffuse_color = color
    obj.data.materials.append(mat)

    return obj


def make_cameras(camera_name, rvec, tvec, camera_matrix):
    # position, direction, rot = calculate_camera_position_direction(rvec, tvec)
    position, rotation = blender_location_rotation_from_opencv(rvec, tvec)
    # print(position, direction)
    # point_obj = create_point(position)
    # direction_point_obj = create_direction_point(position, direction, scale=2.0)
    camera = create_camera(camera_matrix['camera_f'], camera_matrix['camera_c'], position, camera_name, rotation)


def make_cameras_default(camera_name, rvec, tvec):
    position, rotation = blender_location_rotation_from_opencv(rvec, tvec)
    camera = create_camera_default(position, rotation, camera_name)


def export_camera_to_opencv_txt(cam_name, path, file_name):
    cam = bpy.data.objects[cam_name]
    P = get_3x4_P_matrix_from_blender(cam)

    nP = np.matrix(P)
    # path = bpy.path.abspath("//")
    filename = file_name + ".txt"
    file = path + "/" + filename
    np.savetxt(file, nP)
    print(f"Saved to: \"{file}\".")


def export_camera_to_opencv_json(cam_name, path, file_name):
    cam = bpy.data.objects[cam_name]
    _, rvec, tvec = get_3x4_RT_matrix_from_blender(cam)
    R, _ = cv2.Rodrigues(rvec)
    filename = file_name + ".json"
    file = path + "/" + filename
    json_data = OrderedDict()
    json_data['rvec'] = np.array(R.ravel()).tolist()
    json_data['tvec'] = np.array(tvec).tolist()
    rw_json_data(WRITE, file, json_data)
    print(f"Saved to: \"{file}\".")


def rw_json_data(rw_mode, path, data):
    print('json path', path)
    try:
        if rw_mode == 0:
            with open(path, 'r', encoding="utf-8") as rdata:
                json_data = json.load(rdata)
            return json_data
        elif rw_mode == 1:
            with open(path, 'w', encoding="utf-8") as wdata:
                json.dump(data, wdata, ensure_ascii=False, indent="\t")
        else:
            print('not support mode')
    except:
        # print('file r/w error')
        return -1



def draw_line(start, end, color, name):
    m = bpy.data.meshes.new(name)
    o = bpy.data.objects.new(name, m)

    bpy.context.collection.objects.link(o)

    bm = bmesh.new()

    v1 = bm.verts.new(start)
    v2 = bm.verts.new(end)
    e = bm.edges.new([v1, v2])

    bm.to_mesh(m)
    bm.free()

    mat = bpy.data.materials.new(name="line_mat")
    mat.diffuse_color = color
    mat.use_nodes = False
    o.data.materials.append(mat)

    o.hide_render = True  # Hide in rendering
    o.hide_viewport = True
      # Show in viewport



def create_sphere(location, radius, name):
    bpy.ops.mesh.primitive_uv_sphere_add(segments=32, ring_count=16, radius=radius, location=location)
    sphere = bpy.context.object
    sphere.name = name


def find_intersection(p1, p2, obj, epsilon=1e-8):
    intersections = []
    for face in obj.data.polygons:
        vertices = [obj.matrix_world @ obj.data.vertices[v].co for v in face.vertices]
        intersection = geometry.intersect_ray_tri(vertices[0], vertices[1], vertices[2], (p2 - p1), p1, True)
        if intersection:
            dist_to_intersection = (intersection - p1).length
            dist_p1_to_p2 = (p2 - p1).length
            if abs(dist_to_intersection - dist_p1_to_p2) > epsilon:
                intersections.append(intersection)

    return intersections


def quaternion_to_euler_degree(quaternion):
    # Convert quaternion to Euler rotation (radians)
    euler_rad = quaternion.to_euler()

    # Convert radians to degrees
    euler_deg = Vector([math.degrees(axis) for axis in euler_rad])

    return euler_deg


def apply_boolean_modifier(target_obj, cutter_obj, operation='DIFFERENCE'):
    boolean_mod = target_obj.modifiers.new('Boolean', 'BOOLEAN')
    boolean_mod.operation = operation
    boolean_mod.use_self = True
    boolean_mod.object = cutter_obj

    # 모디파이어를 적용합니다.
    target_obj.select_set(True)
    cutter_obj.select_set(True)
    bpy.context.view_layer.objects.active = target_obj
    bpy.ops.object.modifier_apply({"object": target_obj}, modifier=boolean_mod.name)

    # 자른 LED 오브젝트를 삭제합니다.
    bpy.ops.object.select_all(action='DESELECT')
    cutter_obj.select_set(True)
    bpy.ops.object.delete()


def create_filled_emission_circle_on_plane(led_coords, origin_led_dir, circle_radius):
    for i, coord in enumerate(led_coords):
        # Calculate normal and rotation quaternion
        normal = origin_led_dir[i]
        rotation_quat = mathutils.Vector((0, 0, 1)).rotation_difference(mathutils.Vector(normal))

        # Adjust the distance to the origin for the inner circle
        distance_to_o = circle_radius * 0.05  
        location_outer = coord
        location_inner = [coord[0] - distance_to_o * normal[0],
                          coord[1] - distance_to_o * normal[1],
                          coord[2] - distance_to_o * normal[2]]

        # Create an outer circle with Emission shader
        bpy.ops.mesh.primitive_circle_add(radius=circle_radius, location=location_outer, fill_type='NGON')
        outer_circle_obj = bpy.context.object

        # Rotate the outer circle to match the plane
        outer_circle_obj.rotation_euler = rotation_quat.to_euler()



#        # Create new material
#        mat_emission = bpy.data.materials.new(name="Emission_Gradient")
#        mat_emission.use_nodes = True

#        nodes = mat_emission.node_tree.nodes
#        links = mat_emission.node_tree.links

#        # Create nodes
#        emission = nodes.new('ShaderNodeEmission')
#        layer_weight = nodes.new('ShaderNodeLayerWeight')
#        color_ramp = nodes.new('ShaderNodeValToRGB')  # Color Ramp node
#        output = nodes['Material Output']

#        # Connect nodes
#        links.new(layer_weight.outputs['Facing'], color_ramp.inputs['Fac'])  # Connect layer weight to color ramp
#        links.new(color_ramp.outputs['Color'], emission.inputs['Color'])  # Connect color ramp to emission
#        links.new(emission.outputs['Emission'], output.inputs['Surface'])
#        # Color Ramp settings
#        color_ramp.color_ramp.elements[0].color = (1, 1, 1, 1)  # white
#        color_ramp.color_ramp.elements[0].position = 0.055  # position of white color
#        color_ramp.color_ramp.elements[1].color = (0.572, 0.572, 0.572, 1)  # black
#        color_ramp.color_ramp.elements[1].position = 0.170  # position of black color
#        color_ramp.color_ramp.elements.new(0.305)  # grey2
#        color_ramp.color_ramp.elements.new(0.517)  # black
#        color_ramp.color_ramp.elements[2].color = (0.438, 0.438, 0.438, 1)  # grey2
#        color_ramp.color_ramp.elements[3].color = (0, 0, 0, 1)  # black
#        color_ramp.color_ramp.interpolation = 'B_SPLINE'  # Set interpolation to B-Spline

#        # Layer Weight settings
#        layer_weight.inputs['Blend'].default_value = 0.5  # adjust this value to control the effect of the layer weight
#        # Emission strength settings
#        emission.inputs['Strength'].default_value = 5  # Set emission strength

        # Add an Emission shader to the outer circle
        mat_emission = bpy.data.materials.new(name="Emission_Material")
        mat_emission.use_nodes = True
        mat_emission.node_tree.nodes.clear()

        nodes = mat_emission.node_tree.nodes
        links = mat_emission.node_tree.links

        node_output = nodes.new(type='ShaderNodeOutputMaterial')
        node_emission = nodes.new(type='ShaderNodeEmission')

        node_emission.inputs['Strength'].default_value = emission_strength
        node_emission.inputs['Color'].default_value = (255, 255, 255, 1)

        links.new(node_emission.outputs['Emission'], node_output.inputs['Surface'])

        outer_circle_obj.data.materials.append(mat_emission)


        # Create an inner circle with Diffuse shader
        bpy.ops.mesh.primitive_circle_add(radius=(circle_radius * 2.0), location=location_inner, fill_type='NGON')
        inner_circle_obj = bpy.context.object

        # Rotate the inner circle to match the plane and point towards origin
        inner_circle_obj.rotation_euler = rotation_quat.to_euler()

        # Add a Diffuse shader to the inner circle
        mat_diffuse = bpy.data.materials.new(name="Diffuse_Material")
        mat_diffuse.use_nodes = True
        mat_diffuse.node_tree.nodes.clear()

        nodes = mat_diffuse.node_tree.nodes
        links = mat_diffuse.node_tree.links

        node_output = nodes.new(type='ShaderNodeOutputMaterial')
        node_diffuse = nodes.new(type='ShaderNodeBsdfDiffuse')

        node_diffuse.inputs['Color'].default_value = (0, 0, 0, 1)  # Black color

        links.new(node_diffuse.outputs['BSDF'], node_output.inputs['Surface'])

        inner_circle_obj.data.materials.append(mat_diffuse)



def create_filled_emission_sphere(led_coords, origin_led_dir, sphere_radius):
    for i, coord in enumerate(led_coords):
        # Calculate normal and rotation quaternion
        normal = origin_led_dir[i]
        rotation_quat = mathutils.Vector((0, 0, 1)).rotation_difference(mathutils.Vector(normal))
        distance_to_o = sphere_radius * 1.0 
        location_inner = [coord[0] - distance_to_o * normal[0],
                          coord[1] - distance_to_o * normal[1],
                          coord[2] - distance_to_o * normal[2]]

#        # Create a sphere with Emission shader
#        bpy.ops.mesh.primitive_uv_sphere_add(radius=sphere_radius, location=coord)
#        sphere_obj = bpy.context.object

        # push
        bpy.ops.mesh.primitive_uv_sphere_add(radius=sphere_radius, location=coord)
        sphere_obj = bpy.context.object

        # Rotate the sphere to match the normal
        sphere_obj.rotation_euler = rotation_quat.to_euler()

        # Squeeze the sphere along the Z-axis
        # You may need to adjust the scale factor to get the desired amount of squeezing
        sphere_obj.scale = (1, 1, 0.2)  # adjust the Z factor to squeeze the sphere

        # Apply scale
        bpy.ops.object.transform_apply(location=False, rotation=False, scale=True)


        # Apply smooth shading
        bpy.ops.object.shade_smooth()
        # Rotate the sphere to match the normal
        sphere_obj.rotation_euler = rotation_quat.to_euler()

        # Create new material
        mat_emission = bpy.data.materials.new(name="Emission_Gradient")
        mat_emission.use_nodes = True

        nodes = mat_emission.node_tree.nodes
        links = mat_emission.node_tree.links

        # Create nodes
        emission = nodes.new('ShaderNodeEmission')
        layer_weight = nodes.new('ShaderNodeLayerWeight')
        color_ramp = nodes.new('ShaderNodeValToRGB')  # Color Ramp node
        output = nodes['Material Output']

        # Connect nodes
        links.new(layer_weight.outputs['Facing'], color_ramp.inputs['Fac'])  # Connect layer weight to color ramp
        links.new(color_ramp.outputs['Color'], emission.inputs['Color'])  # Connect color ramp to emission
        links.new(emission.outputs['Emission'], output.inputs['Surface'])
        # Color Ramp settings
# Shader 1
#        color_ramp.color_ramp.elements[0].color = (1, 1, 1, 1)  # white
#        color_ramp.color_ramp.elements[0].position = 0.055  # position of white color
#        color_ramp.color_ramp.elements[1].color = (0.572, 0.572, 0.572, 1)  # black
#        color_ramp.color_ramp.elements[1].position = 0.170  # position of black color
#        color_ramp.color_ramp.elements.new(0.305)  # grey2
#        color_ramp.color_ramp.elements.new(0.517)  # black
#        color_ramp.color_ramp.elements[2].color = (0.438, 0.438, 0.438, 1)  # grey2
#        color_ramp.color_ramp.elements[3].color = (0, 0, 0, 1)  # black
#        color_ramp.color_ramp.interpolation = 'B_SPLINE'  # Set interpolation to B-Spline
#        emission.inputs['Strength'].default_value = 5  # Set emission strength

# Shader 2
#        color_ramp.color_ramp.elements[0].color = (1, 1, 1, 1)  # white
#        color_ramp.color_ramp.elements[0].position = 0.141  # position of white color
#        color_ramp.color_ramp.elements[1].color = (0.102, 0.102, 0.102, 1)  # black
#        color_ramp.color_ramp.elements[1].position = 0.263  # position of black color
#        color_ramp.color_ramp.elements.new(0.457)  # grey2
#        color_ramp.color_ramp.elements.new(0.636)  # black
#        color_ramp.color_ramp.elements[2].color = (0.031, 0.031, 0.031, 1)  # grey2
#        color_ramp.color_ramp.elements[3].color = (0, 0, 0, 1)  # black
#        color_ramp.color_ramp.interpolation = 'B_SPLINE'  # Set interpolation to B-Spline
#        emission.inputs['Strength'].default_value = 5  # Set emission strength
# Shader 3
        color_ramp.color_ramp.elements[0].color = (1, 1, 1, 1)  # white
        color_ramp.color_ramp.elements[0].position = 0.460  # position of white color
        color_ramp.color_ramp.elements[1].color = (0.102, 0.102, 0.102, 1)  # black
        color_ramp.color_ramp.elements[1].position = 0.670  # position of black color
        color_ramp.color_ramp.elements.new(0.820)  # grey2
        color_ramp.color_ramp.elements[2].color = (0.031, 0.031, 0.031, 1)  # grey2
        color_ramp.color_ramp.interpolation = 'B_SPLINE'  # Set interpolation to B-Spline
        emission.inputs['Strength'].default_value = 3  # Set emission strength


        # Layer Weight settings
        layer_weight.inputs['Blend'].default_value = 0.5  # adjust this value to control the effect of the layer weight
        # Emission strength settings

        sphere_obj.data.materials.append(mat_emission)  # Apply material to sphere      

      # Create a cylinder with Diffuse shader
        bpy.ops.mesh.primitive_cylinder_add(radius=(sphere_radius * 2.0), depth=sphere_radius * 2.0, location=location_inner)
        inner_circle_obj = bpy.context.object

        # Rotate the cylinder to match the plane and point towards origin
        inner_circle_obj.rotation_euler = rotation_quat.to_euler()

        # Add a Diffuse shader to the inner circle
        mat_diffuse = bpy.data.materials.new(name="Diffuse_Material")
        mat_diffuse.use_nodes = True
        mat_diffuse.node_tree.nodes.clear()

        nodes = mat_diffuse.node_tree.nodes
        links = mat_diffuse.node_tree.links

        node_output = nodes.new(type='ShaderNodeOutputMaterial')
        node_diffuse = nodes.new(type='ShaderNodeBsdfDiffuse')

        node_diffuse.inputs['Color'].default_value = (0, 0, 0, 1)  # Black color

        links.new(node_diffuse.outputs['BSDF'], node_output.inputs['Surface'])

        inner_circle_obj.data.materials.append(mat_diffuse)
          

def pickle_data(rw_mode, path, data):
    import pickle
    import gzip
    try:
        if rw_mode == READ:
            with gzip.open(path, 'rb') as f:
                data = pickle.load(f)
            return data
        elif rw_mode == WRITE:
            with gzip.open(path, 'wb') as f:
                pickle.dump(data, f)
        else:
            print('not support mode')
    except:
        print('file r/w error')
        return ERROR


def make_camera_path(path_type, **kwargs):
    if path_type == 'sine_wave':
        print('make sine_wave')
        # Curve 데이터를 생성합니다.
        curve_data = bpy.data.curves.new(path_type, type='CURVE')
        curve_data.dimensions = '3D'

        # NurbsPath를 만들기 위한 설정을 합니다.
        polyline = curve_data.splines.new('NURBS')

        # 원점으로부터 반지름이 0.3인 구 표면 위를 0, -0.3, 0 부터 0, 0.3, 0 까지 사인파 모양으로 나타냅니다.
        num_points = 500  # 해상도를 높입니다
        polyline.points.add(num_points - 1)  # 포인트의 개수를 설정합니다. 
        radius = 0.3
        amplitude = 0.3  # 펄스의 진폭
        frequency = 5   # 펄스의 주파수

        for i in range(num_points):
            # y는 -0.3에서 0.3까지 선형으로 변화합니다.
            y = -0.3 + i * 0.6 / (num_points - 1)
            
            # angle은 y에 대응하는 각도입니다.
            angle = 2 * math.pi * frequency * y
            
            # xz 평면 위에서 사인파를 그립니다.
            x = radius * math.cos(angle)
            z = amplitude * math.sin(angle)

            polyline.points[i].co = (x, y, z, 1)

        # NurbsPath가 순환형 경로인지 설정합니다. 
        polyline.use_cyclic_u = True

        # 이 Curve 데이터를 Object로 만듭니다.
        curve_object = bpy.data.objects.new(path_type, curve_data)

        # 생성된 Object를 씬에 추가합니다.
        scene = bpy.context.scene
        scene.collection.objects.link(curve_object)

        # Path Animation의 프레임 수를 설정합니다.
        curve_object.data.path_duration = 500        

        animate_path(curve_object, 0, 100, 400)

        # Path Animation 활성화
        curve_object.data.use_path = True

    elif path_type == 'circle_curve':
        print('make circle_curve')
        radius = kwargs.get('radius', 0.3)  # 반지름이 제공되지 않은 경우, 기본값은 0.3으로 설정합니다.

        # Curve 데이터를 생성합니다.
        curve_data = bpy.data.curves.new(path_type, type='CURVE')
        curve_data.dimensions = '3D'

        # NurbsPath를 만들기 위한 설정을 합니다.
        polyline = curve_data.splines.new('NURBS')

        # 원점을 중심으로 반지름이 0.3인 원을 그립니다.
        num_points = 250  # 해상도를 높입니다
        polyline.points.add(num_points - 1)  # 포인트의 개수를 설정합니다. 

        for i in range(num_points):
            # angle은 i에 대응하는 각도입니다.
            angle = 2 * math.pi * i / num_points
            
            # xy 평면 위에서 원을 그립니다.
            x = radius * math.cos(angle)
            y = radius * math.sin(angle)  # 원점을 중심으로 설정합니다.
            z = 0

            polyline.points[i].co = (x, y, z, 1)

        # NurbsPath가 순환형 경로인지 설정합니다. 
        polyline.use_cyclic_u = True

        # 이 Curve 데이터를 Object로 만듭니다.
        curve_object = bpy.data.objects.new(path_type, curve_data)

        # 생성된 Object를 씬에 추가합니다.
        scene = bpy.context.scene
        scene.collection.objects.link(curve_object)

        # Path Animation의 프레임 수를 설정합니다.
        curve_object.data.path_duration = 500        

        animate_path(curve_object, 0, 100, 400)

        # Path Animation 활성화
        curve_object.data.use_path = True

    elif path_type == 'quadrant_segment_circle':
        print('make quadrant_segment_circle')
        quadrant = kwargs['quadrant']  # 사분면 번호
        start_angle_degrees = kwargs['start_angle_degrees']  # 시작 각도
        radius = kwargs.get('radius', 0.3)
        # Path 이름 생성
        path_name = f"quad_circle_curve_{quadrant}_{start_angle_degrees}"

        # Curve 데이터를 생성합니다.
        curve_data = bpy.data.curves.new(path_name, type='CURVE')
        curve_data.dimensions = '3D'

        # NurbsPath를 만들기 위한 설정을 합니다.
        polyline = curve_data.splines.new('NURBS')

        # 원점을 중심으로 반지름이 0.3인 원의 일부를 그립니다.
        num_points = 250  # 해상도를 높입니다
        polyline.points.add(num_points - 1)  # 포인트의 개수를 설정합니다. 

        # 사분면에 따라 시작 각도를 설정합니다.
        if quadrant == 1:
            quadrant_start_angle_degrees = 0
        elif quadrant == 2:
            quadrant_start_angle_degrees = 90
        elif quadrant == 3:
            quadrant_start_angle_degrees = 180
        elif quadrant == 4:
            quadrant_start_angle_degrees = 270
        else:
            raise ValueError("Quadrant should be between 1 and 4.")

        # 각도를 라디안으로 변환합니다.

        start_angle = quadrant_start_angle_degrees * math.pi / 180
        end_angle = start_angle + start_angle_degrees * math.pi / 180  # 입력된 내각만큼 반원을 그립니다.


        for i in range(num_points):
            # angle은 i에 대응하는 각도입니다.
            angle = start_angle + i * (end_angle - start_angle) / (num_points - 1)
            
            # xy 평면 위에서 원을 그립니다.
            x = radius * math.cos(angle)
            y = radius * math.sin(angle)
            z = 0

            polyline.points[i].co = (x, y, z, 1)

        # NurbsPath가 순환형 경로인지 설정합니다. 
        polyline.use_cyclic_u = False  # 원의 일부만 그리므로 순환형 경로는 아닙니다.

        # 이 Curve 데이터를 Object로 만듭니다.
        curve_object = bpy.data.objects.new(path_name, curve_data)

        # 생성된 Object를 씬에 추가합니다.
        scene = bpy.context.scene
        scene.collection.objects.link(curve_object)

        # Path Animation의 프레임 수를 설정합니다.
        curve_object.data.path_duration = 500        

        animate_path(curve_object, 0, 100, 400)

        # Path Animation 활성화
        curve_object.data.use_path = True


def make_camera_follow_path(camera, path_object):
    # Follow Path constraint를 카메라에 추가합니다.
    follow_path = camera.constraints.new(type='FOLLOW_PATH')
    follow_path.target = path_object
    follow_path.use_curve_follow = True
    follow_path.use_curve_radius = True

def make_camera_track_to(camera, target_object):
    # Track To constraint를 카메라에 추가합니다.
    track_to = camera.constraints.new(type='TRACK_TO')
    track_to.target = target_object
    track_to.track_axis = 'TRACK_NEGATIVE_Z'
    track_to.up_axis = 'UP_Y'

def animate_path(curve_object, frame_start, frame_mid, frame_end):
    # Animation data를 만들고 필요하다면 초기화합니다.
    if curve_object.animation_data is None:
        curve_object.animation_data_create()
    
    # Action을 만들고 필요하다면 초기화합니다.
    if curve_object.animation_data.action is None:
        curve_object.animation_data.action = bpy.data.actions.new(name="PathAnimation")
    
    # Curve의 eval_time 속성에 대한 fcurve를 가져오거나 만듭니다.
    fcurves = curve_object.animation_data.action.fcurves
    eval_time_fcurve = next((fcurve for fcurve in fcurves if fcurve.data_path == "eval_time"), None)
    if eval_time_fcurve is None:
        eval_time_fcurve = fcurves.new(data_path="eval_time")
    
    # eval_time을 애니메이션화합니다.
    eval_time_fcurve.keyframe_points.add(4)  # 4개의 keyframe을 추가합니다.
    eval_time_fcurve.keyframe_points[0].co = (frame_start, 0.0)  # 시작 keyframe
    eval_time_fcurve.keyframe_points[0].interpolation = 'BEZIER'  # bezier interpolation

    eval_time_fcurve.keyframe_points[1].co = (frame_mid, 0.5)  # 중간 keyframe
    eval_time_fcurve.keyframe_points[1].interpolation = 'LINEAR'  # linear interpolation

    eval_time_fcurve.keyframe_points[2].co = (frame_end, 1.0)  # 끝 keyframe
    eval_time_fcurve.keyframe_points[2].interpolation = 'BEZIER'  # bezier interpolation

    eval_time_fcurve.keyframe_points[3].co = (500, 1.0)  # 최종 keyframe
    eval_time_fcurve.keyframe_points[3].interpolation = 'BEZIER'  # bezier interpolation


#import numpy as np

#dist_coeffs = np.array([[0.072867], [-0.026268], [0.007135], [-0.000997]], dtype=np.float64)
#import bpy

## Define the distortion coefficients
#k1, k2, p1, p2 = dist_coeffs.flatten()

## Enable use_nodes in the compositor
#bpy.context.scene.use_nodes = True

## Get reference to the node tree
#tree = bpy.context.scene.node_tree

## Clear all nodes
#tree.nodes.clear()

## Create the Render Layers node
#render_layers_node = tree.nodes.new(type='CompositorNodeRLayers')

## Create the Composite node
#composite_node = tree.nodes.new(type='CompositorNodeComposite')

## Create the Movie Distortion node
#movie_distortion_node = tree.nodes.new(type='CompositorNodeMovieDistortion')

## Set the distortion parameters
#movie_distortion_node.distort = 1  # 1 means distortion, -1 means undistortion
#movie_distortion_node.distort_method = 'POLYNOMIAL'
#movie_distortion_node.k1 = k1
#movie_distortion_node.k2 = k2
#movie_distortion_node.k3 = p1
#movie_distortion_node.k4 = p2

## Link the nodes
#links = tree.links
#links.new(render_layers_node.outputs[0], movie_distortion_node.inputs[0])
#links.new(movie_distortion_node.outputs[0], composite_node.inputs[0])
